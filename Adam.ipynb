{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(iris['data'], columns = iris['feature_names'] )\n",
    "target = pd.DataFrame(iris['target'],columns = ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the input predictors and target so that it can be split into training and testing\n",
    "data_target = data.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    50\n",
       "1    50\n",
       "0    50\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_target['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this implementaion, let's make the problem a binary. So considering only '0' and '1' as the target\n",
    "X = np.array(data_target[(data_target['target'] == 0) | (data_target['target'] == 1)].drop('target', axis=1))\n",
    "y = np.array(data_target[(data_target['target'] == 0) | (data_target['target'] == 1)]['target']).reshape(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x: (80, 4)\n",
      "Shape of train_y: (80, 1)\n",
      "Shape of test_x: (20, 4)\n",
      "Shape of test_y: (20, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(X,y,test_size = 0.2)\n",
    "\n",
    "print('Shape of train_x:', train_x.shape)\n",
    "print('Shape of train_y:', train_y.shape)\n",
    "print('Shape of test_x:', test_x.shape)\n",
    "print('Shape of test_y:', test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into 9 mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 9\n",
    "\n",
    "#get the total number of batches\n",
    "batch_count = train_x.shape[0] // batches\n",
    "\n",
    "batch_trainx = []\n",
    "batch_trainy = []\n",
    "\n",
    "for i in range(0, batch_count):    \n",
    "    begin = i * batches\n",
    "    end = (i + 1) * batches\n",
    "\n",
    "    batch_trainx.append(train_x[begin:end])\n",
    "    batch_trainy.append(train_y[begin:end])\n",
    "\n",
    "#when the total count is not exactly divisible by batches\n",
    "left_out = train_x.shape[0] % batches\n",
    "\n",
    "if left_out != 0:    \n",
    "    batch_trainx.append(train_x[end: end + left_out])\n",
    "    batch_trainy.append(train_y[end: end + left_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing the weights for each layer\n",
    "def intialize_weights(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    #initialise the value of weights based on the number of layers\n",
    "    for i in range(1,L-1):\n",
    "        #for the hidden layers we will use He initialisation because of relu activation\n",
    "        parameters['W'+str(i)] = np.random.randn(layer_dims[i-1],layer_dims[i]) * np.sqrt(2/layer_dims[i-1])\n",
    "        parameters['v1'+str(i)] = np.zeros([layer_dims[i-1],layer_dims[i]])\n",
    "        parameters['v2'+str(i)] = np.zeros([layer_dims[i-1],layer_dims[i]])\n",
    "        parameters['b'+str(i)] = np.zeros([1, layer_dims[i]])   \n",
    "        parameters['u1'+str(i)] = np.zeros([1, layer_dims[i]])   \n",
    "        parameters['u2'+str(i)] = np.zeros([1, layer_dims[i]])   \n",
    "        \n",
    "            \n",
    "    #for the last layer we can use Xavier initialisation \n",
    "    parameters['W' + str(i+1)] = np.random.randn(layer_dims[i],layer_dims[i+1]) * np.sqrt(1/layer_dims[i])\n",
    "    parameters['v1' + str(i+1)] = np.zeros([layer_dims[i],layer_dims[i+1]])\n",
    "    parameters['v2' + str(i+1)] = np.zeros([layer_dims[i],layer_dims[i+1]])\n",
    "    parameters['b'+str(i+1)] = np.zeros([1, layer_dims[i+1]])    \n",
    "    parameters['u1'+str(i+1)] = np.zeros([1, layer_dims[i+1]])    \n",
    "    parameters['u2'+str(i+1)] = np.zeros([1, layer_dims[i+1]])    \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "def forward_propagation(layer_dims,train_x,parameters):\n",
    "    \n",
    "    caches = []\n",
    "    Aprev = train_x\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    #forward propagation for all the layers except last layer\n",
    "    for i in range(1,L-1): \n",
    "        W = parameters['W'+ str(i)]\n",
    "        b = parameters['b' + str(i)] \n",
    "        Z = np.dot(Aprev, W) + b  \n",
    "        Aprev = np.maximum(0,Z)       \n",
    "        cache = Aprev, W, b\n",
    "        caches.append(cache)     \n",
    "    \n",
    "    #forward propagation for the last layer\n",
    "    W = parameters['W'+ str(L-1)]\n",
    "    b = parameters['b' + str(L-1)]\n",
    "    Zlast = np.dot(Aprev, W) + b    \n",
    "    Alast = 1/(1 + np.exp(-Zlast))   \n",
    "    cache = Alast, W, b\n",
    "\n",
    "    caches.append(cache)\n",
    "    return caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function calculation\n",
    "def cost_calculate(predict_y,train_y):\n",
    "    m = train_y.shape[0]\n",
    "    cost = -(np.dot(train_y.T, np.log(predict_y)) + np.dot((1-train_y).T, np.log(1-predict_y)))/m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(layer_dims, caches, parameters, train_x, train_y, learning_rate, beta1, beta2, iteration):\n",
    "    #backward propagation for the last layer\n",
    "    #Extract the last array from the caches, as this corresponds to the final output\n",
    "    L = len(caches)    \n",
    "    Acurr,Wcurr,bcurr = caches[L - 1]  \n",
    "    Aprev,Wprev,bprev = caches[L - 2]\n",
    "    v1 = parameters['v1'+str(L)]\n",
    "    u1 = parameters['u1'+str(L)]\n",
    "    v2 = parameters['v2'+str(L)]\n",
    "    u2 = parameters['u2'+str(L)]\n",
    "    epsilon = 10e-8\n",
    "\n",
    "    m = train_y.shape[0]   \n",
    "    \n",
    "    dzprev = (Acurr - train_y)    \n",
    "    dwlast = np.dot(Aprev.T, dzprev)/m    \n",
    "    dblast = np.sum(dzprev, keepdims = True, axis = 0)/m   \n",
    "    dv1last = (beta1 * v1) + ((1-beta1) * dwlast)/(1-np.power(beta1, iteration))\n",
    "    du1last = (beta1 * u1) + ((1-beta1) * dblast)/(1-np.power(beta1, iteration))\n",
    "    dv2last = (beta2 * v2) + ((1-beta2) * np.power(dwlast,2))/(1-np.power(beta2, iteration))\n",
    "    du2last = (beta2 * u2) + ((1-beta2) * np.power(dblast,2))/(1-np.power(beta2, iteration))\n",
    "    \n",
    "    parameters['W' + str(L)]= parameters['W' + str(L)] - (learning_rate * dv1last/np.sqrt(dv2last+epsilon))    \n",
    "    parameters['b' + str(L)]= parameters['b' + str(L)] - (learning_rate * du1last/np.sqrt(du2last+epsilon))   \n",
    "    parameters['v1' + str(L)]= dv1last\n",
    "    parameters['u1' + str(L)]= du1last\n",
    "    parameters['v2' + str(L)]= dv2last\n",
    "    parameters['u2' + str(L)]= du2last\n",
    "            \n",
    "    for i in reversed(range(L-1)):\n",
    "        Anext,Wnext,bnext = caches[i+1]\n",
    "        Acurr,Wcurr,bcurr = caches[i]  \n",
    "        v1 = parameters['v1'+str(i+1)]\n",
    "        u1 = parameters['u1'+str(i+1)]\n",
    "        v2 = parameters['v2'+str(i+1)]\n",
    "        u2 = parameters['u2'+str(i+1)]\n",
    "        \n",
    "        if i == 0:\n",
    "            Aprev = train_x\n",
    "        else:            \n",
    "            Aprev,Wprev,bprev = caches[i-1]\n",
    "                \n",
    "        dzcurr = np.where(Acurr > 0,1,Acurr)                     \n",
    "        dzprev = np.multiply(np.dot(dzprev,Wnext.T), dzcurr)\n",
    "        \n",
    "        dW = np.dot(Aprev.T,dzprev)/m\n",
    "        db = np.sum(dzprev, keepdims = True, axis = 0)/m  \n",
    "        dv1 = (beta1 * v1) + ((1-beta1) * dW)/(1-np.power(beta1, iteration))\n",
    "        du1 = (beta1 * u1) + ((1-beta1) * db)/(1-np.power(beta1, iteration))\n",
    "        dv2 = (beta2 * v2) + ((1-beta2) * np.power(dW,2))/(1-np.power(beta2, iteration))\n",
    "        du2 = (beta2 * u2) + ((1-beta2) * np.power(db,2))/(1-np.power(beta2, iteration))\n",
    "        \n",
    "        parameters['W' + str(i+1)]= parameters['W' + str(i+1)] - (learning_rate * dv1/np.sqrt(dv2+epsilon))\n",
    "        parameters['b' + str(i+1)]= parameters['b' + str(i+1)] - (learning_rate * du1/np.sqrt(du2+epsilon))    \n",
    "        parameters['v1' + str(i+1)]= dv1\n",
    "        parameters['u1' + str(i+1)]= du1\n",
    "        parameters['v2' + str(i+1)]= dv2\n",
    "        parameters['u2' + str(i+1)]= du2\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_model(layer_dims, batch_trainx, batch_trainy, learning_rate, beta1, beta2, iteration):\n",
    "        \n",
    "    L = len(layer_dims)\n",
    "    # Intialize the weights\n",
    "    parameters = intialize_weights(layer_dims)\n",
    "    act_iteration = 0\n",
    "    \n",
    "    for i in range(iterations):        \n",
    "        for j in range(len(batch_trainx)):            \n",
    "            #forward propagation\n",
    "            caches = forward_propagation(layer_dims,batch_trainx[j],parameters)\n",
    "        \n",
    "            #calculate the cost \n",
    "            A,W,b = caches[-1]\n",
    "            cost = cost_calculate(A,batch_trainy[j])\n",
    "            act_iteration += 1\n",
    "            parameters = backward_propagation(layer_dims, caches, parameters,batch_trainx[j], batch_trainy[j], learning_rate, beta1, beta2, act_iteration)\n",
    "            \n",
    "        if i%1000 == 0:\n",
    "            print('The cost after iteration {}: {}'.format(i, np.squeeze(cost)))\n",
    "        #backward propagation            \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after iteration 0: 0.3845395014051043\n",
      "The cost after iteration 1000: 0.00016910230289556455\n",
      "The cost after iteration 2000: 2.2398821097787057e-05\n",
      "The cost after iteration 3000: 1.1788991810088925e-05\n",
      "The cost after iteration 4000: 7.994388214255581e-06\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [4,5,3,1]\n",
    "learning_rate = 0.001\n",
    "iterations = 5000\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "parameters = complete_model(layer_dims, batch_trainx, batch_trainy, learning_rate, beta1, beta2, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.82274379e-06]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches = forward_propagation(layer_dims,test_x,parameters)\n",
    "A,W,b = caches[-1]\n",
    "cost = cost_calculate(A, test_y)\n",
    "cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSprop using keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (4, ),kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(50,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1,kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/50\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.0469 - accuracy: 0.5375 - val_loss: 0.6222 - val_accuracy: 0.5500\n",
      "Epoch 2/50\n",
      "80/80 [==============================] - 0s 113us/step - loss: 0.4517 - accuracy: 0.9625 - val_loss: 0.2543 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.2046 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.1165 - accuracy: 1.0000 - val_loss: 0.1228 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0735 - accuracy: 1.0000 - val_loss: 0.0781 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.0438 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "80/80 [==============================] - 0s 188us/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "80/80 [==============================] - 0s 213us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "80/80 [==============================] - 0s 188us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2a18d2347c8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=9, validation_data=(test_x, test_y), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
