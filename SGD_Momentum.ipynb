{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(iris['data'], columns = iris['feature_names'] )\n",
    "target = pd.DataFrame(iris['target'],columns = ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the input predictors and target so that it can be split into training and testing\n",
    "data_target = data.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    50\n",
       "1    50\n",
       "0    50\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_target['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this implementaion, let's make the problem a binary. So considering only '0' and '1' as the target\n",
    "X = np.array(data_target[(data_target['target'] == 0) | (data_target['target'] == 1)].drop('target', axis=1))\n",
    "y = np.array(data_target[(data_target['target'] == 0) | (data_target['target'] == 1)]['target']).reshape(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x: (80, 4)\n",
      "Shape of train_y: (80, 1)\n",
      "Shape of test_x: (20, 4)\n",
      "Shape of test_y: (20, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(X,y,test_size = 0.2)\n",
    "\n",
    "print('Shape of train_x:', train_x.shape)\n",
    "print('Shape of train_y:', train_y.shape)\n",
    "print('Shape of test_x:', test_x.shape)\n",
    "print('Shape of test_y:', test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into 9 mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 9\n",
    "\n",
    "#get the total number of batches\n",
    "batch_count = train_x.shape[0] // batches\n",
    "\n",
    "batch_trainx = []\n",
    "batch_trainy = []\n",
    "\n",
    "for i in range(0, batch_count):    \n",
    "    begin = i * batches\n",
    "    end = (i + 1) * batches\n",
    "\n",
    "    batch_trainx.append(train_x[begin:end])\n",
    "    batch_trainy.append(train_y[begin:end])\n",
    "\n",
    "#when the total count is not exactly divisible by batches\n",
    "left_out = train_x.shape[0] % batches\n",
    "\n",
    "if left_out != 0:    \n",
    "    batch_trainx.append(train_x[end: end + left_out])\n",
    "    batch_trainy.append(train_y[end: end + left_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing the weights for each layer\n",
    "def intialize_weights(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    #initialise the value of weights based on the number of layers\n",
    "    for i in range(1,L-1):\n",
    "        #for the hidden layers we will use He initialisation because of relu activation\n",
    "        parameters['W'+str(i)] = np.random.randn(layer_dims[i-1],layer_dims[i]) * np.sqrt(2/layer_dims[i-1])\n",
    "        parameters['v'+str(i)] = np.zeros([layer_dims[i-1],layer_dims[i]])\n",
    "        parameters['b'+str(i)] = np.zeros([1, layer_dims[i]])   \n",
    "        parameters['u'+str(i)] = np.zeros([1, layer_dims[i]])   \n",
    "        \n",
    "            \n",
    "    #for the last layer we can use Xavier initialisation \n",
    "    parameters['W' + str(i+1)] = np.random.randn(layer_dims[i],layer_dims[i+1]) * np.sqrt(1/layer_dims[i])\n",
    "    parameters['v' + str(i+1)] = np.zeros([layer_dims[i],layer_dims[i+1]])\n",
    "    parameters['b'+str(i+1)] = np.zeros([1, layer_dims[i+1]])    \n",
    "    parameters['u'+str(i+1)] = np.zeros([1, layer_dims[i+1]])    \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "def forward_propagation(layer_dims,train_x,parameters):\n",
    "    \n",
    "    caches = []\n",
    "    Aprev = train_x\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    #forward propagation for all the layers except last layer\n",
    "    for i in range(1,L-1): \n",
    "        W = parameters['W'+ str(i)]\n",
    "        b = parameters['b' + str(i)] \n",
    "        Z = np.dot(Aprev, W) + b  \n",
    "        Aprev = np.maximum(0,Z)       \n",
    "        cache = Aprev, W, b\n",
    "        caches.append(cache)     \n",
    "    \n",
    "    #forward propagation for the last layer\n",
    "    W = parameters['W'+ str(L-1)]\n",
    "    b = parameters['b' + str(L-1)]\n",
    "    Zlast = np.dot(Aprev, W) + b    \n",
    "    Alast = 1/(1 + np.exp(-Zlast))   \n",
    "    cache = Alast, W, b\n",
    "\n",
    "    caches.append(cache)\n",
    "    return caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function calculation\n",
    "def cost_calculate(predict_y,train_y):\n",
    "    m = train_y.shape[0]\n",
    "    cost = -(np.dot(train_y.T, np.log(predict_y)) + np.dot((1-train_y).T, np.log(1-predict_y)))/m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(layer_dims, caches, parameters, train_x, train_y, learning_rate, beta, iteration):\n",
    "    #backward propagation for the last layer\n",
    "    #Extract the last array from the caches, as this corresponds to the final output\n",
    "    L = len(caches)    \n",
    "    Acurr,Wcurr,bcurr = caches[L - 1]  \n",
    "    Aprev,Wprev,bprev = caches[L - 2]\n",
    "    v = parameters['v'+str(L)]\n",
    "    u = parameters['u'+str(L)]\n",
    "\n",
    "    m = train_y.shape[0]   \n",
    "    \n",
    "    dzprev = (Acurr - train_y)    \n",
    "    dwlast = np.dot(Aprev.T, dzprev)/m    \n",
    "    dblast = np.sum(dzprev, keepdims = True, axis = 0)/m   \n",
    "    dvlast = (beta * v) + ((1-beta) * dwlast)/(1-np.power(beta, iteration))\n",
    "    dulast = (beta * u) + ((1-beta) * dblast)/(1-np.power(beta, iteration))\n",
    "    \n",
    "    parameters['W' + str(L)]= parameters['W' + str(L)] - (learning_rate * dvlast)    \n",
    "    parameters['b' + str(L)]= parameters['b' + str(L)] - (learning_rate * dulast)   \n",
    "    parameters['v' + str(L)]= dvlast\n",
    "    parameters['u' + str(L)]= dulast\n",
    "            \n",
    "    for i in reversed(range(L-1)):\n",
    "        Anext,Wnext,bnext = caches[i+1]\n",
    "        Acurr,Wcurr,bcurr = caches[i]  \n",
    "        v = parameters['v'+str(i+1)]\n",
    "        u = parameters['u'+str(i+1)]\n",
    "        \n",
    "        if i == 0:\n",
    "            Aprev = train_x\n",
    "        else:            \n",
    "            Aprev,Wprev,bprev = caches[i-1]\n",
    "                \n",
    "        dzcurr = np.where(Acurr > 0,1,Acurr)                     \n",
    "        dzprev = np.multiply(np.dot(dzprev,Wnext.T), dzcurr)\n",
    "        \n",
    "        dW = np.dot(Aprev.T,dzprev)/m\n",
    "        db = np.sum(dzprev, keepdims = True, axis = 0)/m  \n",
    "        dv = (beta * v) + ((1-beta) * dW)/(1-np.power(beta, iteration))\n",
    "        du = (beta * u) + ((1-beta) * db)/(1-np.power(beta, iteration))\n",
    "        \n",
    "        parameters['W' + str(i+1)]= parameters['W' + str(i+1)] - (learning_rate * dv)\n",
    "        parameters['b' + str(i+1)]= parameters['b' + str(i+1)] - (learning_rate * du)    \n",
    "        parameters['v' + str(i+1)]= dv\n",
    "        parameters['u' + str(i+1)]= du\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_model(layer_dims, batch_trainx, batch_trainy, learning_rate, iterations, beta):\n",
    "        \n",
    "    L = len(layer_dims)\n",
    "    # Intialize the weights\n",
    "    parameters = intialize_weights(layer_dims)\n",
    "    act_iteration = 0\n",
    "    \n",
    "    for i in range(iterations):        \n",
    "        for j in range(len(batch_trainx)):            \n",
    "            #forward propagation\n",
    "            caches = forward_propagation(layer_dims,batch_trainx[j],parameters)\n",
    "        \n",
    "            #calculate the cost \n",
    "            A,W,b = caches[-1]\n",
    "            cost = cost_calculate(A,batch_trainy[j])\n",
    "            act_iteration += 1\n",
    "            parameters = backward_propagation(layer_dims, caches, parameters,batch_trainx[j], batch_trainy[j], learning_rate, beta, act_iteration)\n",
    "            \n",
    "        if i%1000 == 0:\n",
    "            print('The cost after iteration {}: {}'.format(i, np.squeeze(cost)))\n",
    "        #backward propagation            \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after iteration 0: 0.37198811438904533\n",
      "The cost after iteration 1000: 0.000733552926585688\n",
      "The cost after iteration 2000: 0.00036638842016319107\n",
      "The cost after iteration 3000: 0.000244082220490538\n",
      "The cost after iteration 4000: 0.00018296735587746365\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [4,5,3,1]\n",
    "learning_rate = 0.15\n",
    "iterations = 5000\n",
    "beta = 0.9\n",
    "parameters = complete_model(layer_dims, batch_trainx, batch_trainy, learning_rate, iterations, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00013839]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches = forward_propagation(layer_dims,test_x,parameters)\n",
    "A,W,b = caches[-1]\n",
    "cost = cost_calculate(A, test_y)\n",
    "cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD using keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape = (4, ),kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(50,kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1,kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr = 0.001)\n",
    "model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/50\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 1.3097 - accuracy: 0.5125 - val_loss: 0.9812 - val_accuracy: 0.4500\n",
      "Epoch 2/50\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.8018 - accuracy: 0.5125 - val_loss: 0.7112 - val_accuracy: 0.4500\n",
      "Epoch 3/50\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.6019 - accuracy: 0.5500 - val_loss: 0.5308 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.4702 - accuracy: 0.9625 - val_loss: 0.4296 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.3823 - accuracy: 1.0000 - val_loss: 0.3613 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.3234 - accuracy: 1.0000 - val_loss: 0.3115 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.2802 - accuracy: 1.0000 - val_loss: 0.2772 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.2479 - accuracy: 1.0000 - val_loss: 0.2488 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.2223 - accuracy: 1.0000 - val_loss: 0.2268 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.2027 - accuracy: 1.0000 - val_loss: 0.2087 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.1866 - accuracy: 1.0000 - val_loss: 0.1936 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.1720 - accuracy: 1.0000 - val_loss: 0.1805 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.1604 - accuracy: 1.0000 - val_loss: 0.1688 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.1496 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.1399 - accuracy: 1.0000 - val_loss: 0.1502 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.1325 - accuracy: 1.0000 - val_loss: 0.1429 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.1255 - accuracy: 1.0000 - val_loss: 0.1357 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.1187 - accuracy: 1.0000 - val_loss: 0.1292 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.1130 - accuracy: 1.0000 - val_loss: 0.1235 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.1077 - accuracy: 1.0000 - val_loss: 0.1181 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.1026 - accuracy: 1.0000 - val_loss: 0.1131 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.0983 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0941 - accuracy: 1.0000 - val_loss: 0.1046 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0905 - accuracy: 1.0000 - val_loss: 0.1009 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0868 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.0839 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0807 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0779 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0753 - accuracy: 1.0000 - val_loss: 0.0856 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.0727 - accuracy: 1.0000 - val_loss: 0.0831 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.0704 - accuracy: 1.0000 - val_loss: 0.0808 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.0682 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0662 - accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0643 - accuracy: 1.0000 - val_loss: 0.0744 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.0608 - accuracy: 1.0000 - val_loss: 0.0707 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "80/80 [==============================] - 0s 113us/step - loss: 0.0592 - accuracy: 1.0000 - val_loss: 0.0690 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.0576 - accuracy: 1.0000 - val_loss: 0.0674 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.0561 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.0644 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.0534 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.0522 - accuracy: 1.0000 - val_loss: 0.0617 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.0509 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0476 - accuracy: 1.0000 - val_loss: 0.0568 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.0466 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 0.0547 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.0537 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.0439 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c951a52948>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=9, validation_data=(test_x, test_y), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
